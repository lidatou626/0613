import streamlit as st
import pandas as pd
import numpy as np
import time
import pickle
from surprise import Reader, Dataset, SVD
from surprise.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import jieba
from snownlp import SnowNLP

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="5Açº§æ™¯åŒºæ¨èç³»ç»Ÿ",
    page_icon="ğŸï¸",
    layout="wide"
)

# é¡µé¢æ ‡é¢˜å’Œä»‹ç»
st.title("å…¨å›½5Açº§æ™¯åŒºæ¨èç³»ç»Ÿ")
st.markdown("åŸºäºç”¨æˆ·è¯„è®ºçš„æƒ…æ„Ÿåˆ†æå’Œå†…å®¹ç›¸ä¼¼åº¦çš„æ™¯åŒºæ¨è")

# åˆå§‹åŒ–çŠ¶æ€å˜é‡
if 'model_ready' not in st.session_state:
    st.session_state.model_ready = False
if 'data_ready' not in st.session_state:
    st.session_state.data_ready = False


# æ•°æ®åŠ è½½å’Œé¢„å¤„ç†å‡½æ•°
@st.cache_data
def load_and_preprocess_data():
    try:
        # åŠ è½½æ•°æ®
        excel_file = pd.ExcelFile('å…¨å›½5Açº§æ™¯åŒº.xlsx')
        df_scenic = excel_file.parse('5A')
        df_comment = pd.read_excel('æ™¯åŒºè¯„è®ºæ•°æ®é›†.xlsx')

        # æ•°æ®é¢„å¤„ç†
        df_scenic = df_scenic.dropna(subset=['dth_title'])
        df_comment = df_comment.dropna(subset=['æ™¯åŒºåç§°'])
        merged_df = pd.merge(df_scenic, df_comment, left_on='dth_title', right_on='æ™¯åŒºåç§°', how='outer')

        # æå–å…³é”®è¯å’Œæƒ…æ„Ÿåˆ†æ
        def extract_keywords(text):
            if isinstance(text, float) and pd.isna(text):
                return []
            return jieba.lcut(text)

        def get_sentiment_score(text):
            if isinstance(text, float) and pd.isna(text):
                return None
            return SnowNLP(text).sentiments

        merged_df['å…³é”®è¯'] = merged_df['è¯„è®ºå†…å®¹'].apply(extract_keywords)
        merged_df['æƒ…æ„Ÿå¾—åˆ†'] = merged_df['è¯„è®ºå†…å®¹'].apply(get_sentiment_score)
        merged_df['æ˜ å°„è¯„åˆ†'] = merged_df['æƒ…æ„Ÿå¾—åˆ†'] * 4 + 1  # æ˜ å°„åˆ°1-5åˆ†

        st.session_state.data_ready = True
        return merged_df

    except Exception as e:
        st.error(f"æ•°æ®åŠ è½½å‡ºé”™: {e}")
        return None


# æ¨¡å‹è®­ç»ƒå‡½æ•°
@st.cache_resource
def train_model(df):
    try:
        # åˆ›å»ºReaderå’ŒDataset
        reader = Reader(rating_scale=(1, 5))
        data = Dataset.load_from_df(df[['ç”¨æˆ·ID', 'æ™¯åŒºåç§°', 'æ˜ å°„è¯„åˆ†']], reader)

        # åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
        trainset, testset = train_test_split(data, test_size=0.2, random_state=42)

        # è®­ç»ƒSVDæ¨¡å‹
        start_time = time.time()
        algo = SVD(
            n_factors=50,
            n_epochs=20,
            lr_all=0.005,
            reg_all=0.02,
            random_state=42
        )
        algo.fit(trainset)
        training_time = time.time() - start_time

        # å†…å®¹æ¨è - åŸºäºTF-IDF
        scenic_reviews = df.groupby('æ™¯åŒºåç§°')['è¯„è®ºå†…å®¹'].agg(lambda x: ' '.join(x)).reset_index()

        # è‡ªå®šä¹‰åˆ†è¯å™¨
        def chinese_tokenizer(text):
            return jieba.lcut(text)

        vectorizer = TfidfVectorizer(
            tokenizer=chinese_tokenizer,
            stop_words=['çš„', 'äº†', 'å’Œ', 'æ˜¯', 'åœ¨'],
            ngram_range=(1, 2),
            max_features=5000
        )

        tfidf_matrix = vectorizer.fit_transform(scenic_reviews['è¯„è®ºå†…å®¹'])
        similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)
        scenic_index = {name: i for i, name in enumerate(scenic_reviews['æ™¯åŒºåç§°'])}

        st.session_state.model_ready = True
        return algo, scenic_reviews, similarity_matrix, scenic_index, training_time

    except Exception as e:
        st.error(f"æ¨¡å‹è®­ç»ƒå‡ºé”™: {e}")
        return None, None, None, None, 0


# å†…å®¹æ¨èå‡½æ•°
def get_similar_scenics(scenic_name, scenic_reviews, similarity_matrix, scenic_index, top_n=5):
    if scenic_name not in scenic_index:
        return pd.DataFrame({"æ™¯åŒºåç§°": [f"æŠ±æ­‰ï¼Œæ™¯åŒº '{scenic_name}' ä¸åœ¨æ•°æ®é›†ä¸­ã€‚"], "ç›¸ä¼¼åº¦å¾—åˆ†": [0]})

    idx = scenic_index[scenic_name]
    sim_scores = list(enumerate(similarity_matrix[idx]))
    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)
    top_scenics = sim_scores[1:top_n + 1]

    result = []
    for i, score in top_scenics:
        result.append({
            'æ™¯åŒºåç§°': scenic_reviews.iloc[i]['æ™¯åŒºåç§°'],
            'ç›¸ä¼¼åº¦å¾—åˆ†': score
        })

    return pd.DataFrame(result)


# æ··åˆæ¨èå‡½æ•°
def hybrid_recommend(user_id, scenic_name, df, algo, scenic_reviews, similarity_matrix, scenic_index, top_n=5,
                     content_weight=0.7, collab_weight=0.3):
    # å†…å®¹æ¨è
    content_rec = get_similar_scenics(scenic_name, scenic_reviews, similarity_matrix, scenic_index, top_n * 2)

    # ååŒè¿‡æ»¤æ¨è
    items = df['æ™¯åŒºåç§°'].unique()
    user_items = df[df['ç”¨æˆ·ID'] == user_id]['æ™¯åŒºåç§°'].tolist()
    unrated_items = [item for item in items if item not in user_items]

    predictions = []
    for item in unrated_items:
        pred = algo.predict(user_id, item)
        predictions.append((item, pred.est))

    collab_rec = pd.DataFrame(predictions, columns=['æ™¯åŒºåç§°', 'é¢„æµ‹è¯„åˆ†'])
    collab_rec = collab_rec.sort_values('é¢„æµ‹è¯„åˆ†', ascending=False).head(top_n * 2)

    # æ··åˆæ¨è
    merged_rec = pd.merge(content_rec, collab_rec, on='æ™¯åŒºåç§°', how='outer')
    merged_rec['ç»¼åˆå¾—åˆ†'] = (content_weight * merged_rec['ç›¸ä¼¼åº¦å¾—åˆ†'].fillna(0) +
                              collab_weight * merged_rec['é¢„æµ‹è¯„åˆ†'].fillna(0))

    return merged_rec.sort_values('ç»¼åˆå¾—åˆ†', ascending=False).head(top_n)


# ä¸»ç¨‹åºæµç¨‹
with st.spinner("æ­£åœ¨åŠ è½½æ•°æ®å’Œè®­ç»ƒæ¨¡å‹..."):
    df = load_and_preprocess_data()

    if df is not None:
        algo, scenic_reviews, similarity_matrix, scenic_index, training_time = train_model(df)

        if algo is not None:
            st.success(f"æ•°æ®åŠ è½½å’Œæ¨¡å‹è®­ç»ƒå®Œæˆï¼è®­ç»ƒè€—æ—¶: {training_time:.2f}ç§’")

            # æ˜¾ç¤ºæ•°æ®ç»Ÿè®¡ä¿¡æ¯
            st.subheader("æ•°æ®ç»Ÿè®¡ä¿¡æ¯")
            col1, col2 = st.columns(2)
            col1.metric("æ™¯åŒºæ•°é‡", len(df['æ™¯åŒºåç§°'].unique()))
            col2.metric("ç”¨æˆ·è¯„è®ºæ•°é‡", len(df))

            # æ˜¾ç¤ºä¸€äº›çƒ­é—¨æ™¯åŒº
            st.subheader("çƒ­é—¨æ™¯åŒº")
            popular_scenics = df.groupby('æ™¯åŒºåç§°')['ç”¨æˆ·ID'].count().sort_values(ascending=False).head(5)
            st.bar_chart(popular_scenics)

            # æ¨èç³»ç»Ÿç•Œé¢
            st.subheader("æ™¯åŒºæ¨è")
            recommendation_type = st.selectbox(
                "æ¨èç±»å‹",
                ["åŸºäºå†…å®¹çš„æ¨è", "åŸºäºç”¨æˆ·çš„æ¨è", "æ··åˆæ¨è"]
            )

            if recommendation_type == "åŸºäºå†…å®¹çš„æ¨è":
                selected_scenic = st.selectbox(
                    "é€‰æ‹©æ™¯åŒº",
                    sorted(df['æ™¯åŒºåç§°'].unique())
                )
                top_n = st.slider("æ¨èæ•°é‡", 1, 10, 5)

                if st.button("è·å–æ¨è"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆæ¨è..."):
                        recommendations = get_similar_scenics(
                            selected_scenic,
                            scenic_reviews,
                            similarity_matrix,
                            scenic_index,
                            top_n
                        )
                        st.write(f"ä¸ **{selected_scenic}** ç›¸ä¼¼çš„æ™¯åŒº:")
                        st.dataframe(recommendations.style.format({"ç›¸ä¼¼åº¦å¾—åˆ†": "{:.2f}"}))

            elif recommendation_type == "åŸºäºç”¨æˆ·çš„æ¨è":
                user_id = st.text_input("è¾“å…¥ç”¨æˆ·ID", "11111")
                top_n = st.slider("æ¨èæ•°é‡", 1, 10, 5)

                if st.button("è·å–æ¨è"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆæ¨è..."):
                        # è·å–ç”¨æˆ·å·²ç»è¯„ä»·è¿‡çš„æ™¯åŒº
                        user_scenics = df[df['ç”¨æˆ·ID'] == user_id]['æ™¯åŒºåç§°'].unique()

                        if len(user_scenics) == 0:
                            st.warning(f"ç”¨æˆ· {user_id} æ²¡æœ‰è¯„ä»·è®°å½•ï¼Œæ— æ³•è¿›è¡Œä¸ªæ€§åŒ–æ¨èã€‚")
                        else:
                            st.write(f"ç”¨æˆ· {user_id} å·²è¯„ä»·çš„æ™¯åŒº:")
                            st.write(", ".join(user_scenics))

                            # ç”Ÿæˆæ¨è
                            items = df['æ™¯åŒºåç§°'].unique()
                            user_items = df[df['ç”¨æˆ·ID'] == user_id]['æ™¯åŒºåç§°'].tolist()
                            unrated_items = [item for item in items if item not in user_items]

                            predictions = []
                            for item in unrated_items:
                                pred = algo.predict(user_id, item)
                                predictions.append((item, pred.est))

                            recommendations = pd.DataFrame(predictions, columns=['æ™¯åŒºåç§°', 'é¢„æµ‹è¯„åˆ†'])
                            recommendations = recommendations.sort_values('é¢„æµ‹è¯„åˆ†', ascending=False).head(top_n)

                            st.write(f"ä¸ºç”¨æˆ· {user_id} æ¨èçš„æ™¯åŒº:")
                            st.dataframe(recommendations.style.format({"é¢„æµ‹è¯„åˆ†": "{:.2f}"}))

            else:  # æ··åˆæ¨è
                user_id = st.text_input("è¾“å…¥ç”¨æˆ·ID", "11111")
                selected_scenic = st.selectbox(
                    "é€‰æ‹©å‚è€ƒæ™¯åŒº",
                    sorted(df['æ™¯åŒºåç§°'].unique())
                )
                top_n = st.slider("æ¨èæ•°é‡", 1, 10, 5)
                content_weight = st.slider("å†…å®¹æ¨èæƒé‡", 0.0, 1.0, 0.7, 0.1)
                collab_weight = st.slider("ååŒè¿‡æ»¤æƒé‡", 0.0, 1.0, 0.3, 0.1)

                if st.button("è·å–æ¨è"):
                    with st.spinner("æ­£åœ¨ç”Ÿæˆæ··åˆæ¨è..."):
                        recommendations = hybrid_recommend(
                            user_id,
                            selected_scenic,
                            df,
                            algo,
                            scenic_reviews,
                            similarity_matrix,
                            scenic_index,
                            top_n,
                            content_weight,
                            collab_weight
                        )

                        st.write(f"æ··åˆæ¨èç»“æœ (ç”¨æˆ· {user_id} å¯èƒ½å–œæ¬¢çš„ç±»ä¼¼ **{selected_scenic}** çš„æ™¯åŒº):")
                        st.dataframe(recommendations[['æ™¯åŒºåç§°', 'ç›¸ä¼¼åº¦å¾—åˆ†', 'é¢„æµ‹è¯„åˆ†', 'ç»¼åˆå¾—åˆ†']].style.format({
                            "ç›¸ä¼¼åº¦å¾—åˆ†": "{:.2f}",
                            "é¢„æµ‹è¯„åˆ†": "{:.2f}",
                            "ç»¼åˆå¾—åˆ†": "{:.2f}"
                        }))

            # æ™¯åŒºè¯¦æƒ…æŸ¥çœ‹
            st.subheader("æ™¯åŒºè¯¦æƒ…")
            selected_scenic_detail = st.selectbox(
                "é€‰æ‹©æ™¯åŒºæŸ¥çœ‹è¯¦æƒ…",
                sorted(df['æ™¯åŒºåç§°'].unique())
            )

            if st.button("æŸ¥çœ‹è¯¦æƒ…"):
                scenic_details = df[df['æ™¯åŒºåç§°'] == selected_scenic_detail].iloc[0]
                st.write(f"### {scenic_details['æ™¯åŒºåç§°']}")

                col1, col2 = st.columns(2)
                col1.metric("å¹³å‡è¯„åˆ†", f"{scenic_details['æ˜ å°„è¯„åˆ†']:.1f}/5.0")
                col1.metric("è¯„è®ºæ•°é‡", len(df[df['æ™¯åŒºåç§°'] == selected_scenic_detail]))

                st.write("**æ™¯åŒºç®€ä»‹**")
                st.write(scenic_details.get('æ™¯åŒºç®€ä»‹', 'æš‚æ— ç®€ä»‹'))

                st.write("**çƒ­é—¨è¯„è®º**")
                comments = df[df['æ™¯åŒºåç§°'] == selected_scenic_detail]['è¯„è®ºå†…å®¹'].dropna().head(3)
                for i, comment in enumerate(comments):
                    st.markdown(f"> {comment}")

                st.write("**å…³é”®è¯**")
                keywords = scenic_details.get('å…³é”®è¯', [])
                if keywords:
                    st.markdown(" ".join([f"#{word}" for word in keywords[:10]]))
else:
st.error("æ•°æ®åŠ è½½æˆ–æ¨¡å‹è®­ç»ƒå¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶å’Œä¾èµ–åº“ã€‚")
# Press the green button in the gutter to run the script.
if __name__ == '__main__':
    print_hi('PyCharm')

# See PyCharm help at https://www.jetbrains.com/help/pycharm/
